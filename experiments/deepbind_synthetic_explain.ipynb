{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aba14cc-55f8-4775-848e-5c6e4849443f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np, os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "from src.utils.synthetic_seqdata import download_data, load_data, sequence_string_to_one_hot\n",
    "from src.models import DeepBindCNN\n",
    "from src.trainer import Trainer\n",
    "from src.utils.datasets import DNASequenceDataset\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from src.utils import metrics\n",
    "from src.explain import Explainer\n",
    "\n",
    "import matplotlib as mpl \n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1305f2-450c-415d-b48a-dc20c8e25503",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000,\n",
       " {'train': array([1., 1., 0., ..., 1., 0., 1.], dtype=float32),\n",
       "  'valid': array([0., 1., 1., ..., 0., 0., 1.], dtype=float32),\n",
       "  'test': array([0., 1., 0., ..., 1., 1., 1.], dtype=float32)},\n",
       " 14000,\n",
       " 2000,\n",
       " 4000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data \n",
    "savedir = \"./data\"\n",
    "# _=download_data(savedir)\n",
    "Xs, Ys = load_data(savedir=savedir)\n",
    "len(Xs['train']), Ys, len(Ys['train']), len(Ys['valid']), len(Ys['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217bc5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49c93654",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"deepbind\",\n",
    "    \"dataset\": \"synthetic data\",\n",
    "    \"epochs\": 35,\n",
    "    \"patience\": 3,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30412a0c-b264-499b-9b59-c0e95f87496d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepBindCNN(\n",
      "  (conv1): Conv1d(4, 16, kernel_size=(3,), stride=(1,))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# set up datasets\n",
    "datasets = {}\n",
    "for k in Xs:\n",
    "    datasets[k] = DNASequenceDataset(sequences=Xs[k], labels=Ys[k], alphabet=\"ACGT\")\n",
    "\n",
    "# set up dataloaders \n",
    "loaders = {}\n",
    "for k, dataset in datasets.items():\n",
    "    if k == 'train':\n",
    "        loaders[k] = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    else:\n",
    "        loaders[k] = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "# set up the model, lossfn, optimizer, trainer \n",
    "model = DeepBindCNN(input_size=4, output_size=1, kernel_size=3)\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "159df4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000, -0.0583, -0.0000, -0.0000,  0.0000,  0.0504,\n",
       "          -0.1874,  0.0000,  0.0000, -0.0000, -0.0963, -0.0000, -0.0000,\n",
       "          -0.0000, -0.0376,  0.0516, -0.0602, -0.0000,  0.0000,  0.0000,\n",
       "          -0.0000, -0.1013,  0.0000, -0.0000,  0.0000,  0.0000, -0.0504,\n",
       "          -0.1115, -0.0607,  0.0000, -0.0000,  0.0000,  0.0000, -0.1307,\n",
       "          -0.0000,  0.0000, -0.0722, -0.0000,  0.0000,  0.0000, -0.1384,\n",
       "          -0.0000,  0.0000,  0.0000, -0.1384, -0.0000,  0.0000, -0.0682,\n",
       "          -0.0000, -0.0000, -0.0000,  0.0000,  0.0178, -0.1629,  0.0000,\n",
       "          -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
       "          -0.0673,  0.0000,  0.0000, -0.0000, -0.1339, -0.0000, -0.0000,\n",
       "           0.0000, -0.0000, -0.0000, -0.0963,  0.0000, -0.0000, -0.0000,\n",
       "          -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
       "          -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
       "          -0.0272, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
       "          -0.0000,  0.0000,  0.0000, -0.1627,  0.0000, -0.0000,  0.0000,\n",
       "           0.0192, -0.1506,  0.0000,  0.0000, -0.0716, -0.1477,  0.0000,\n",
       "          -0.0000, -0.0000, -0.0000,  0.0000, -0.1224, -0.0000, -0.0000,\n",
       "           0.0000, -0.0000, -0.1174, -0.0513,  0.0000, -0.0000, -0.0000,\n",
       "           0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.1672,  0.0000,\n",
       "           0.0000,  0.0000, -0.1133, -0.0000, -0.0000, -0.0000,  0.0000,\n",
       "          -0.0902,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          -0.0000, -0.0000, -0.0394,  0.0000,  0.0000, -0.0783,  0.0000,\n",
       "          -0.0000,  0.0000,  0.0000, -0.1747,  0.0000, -0.0000,  0.0000,\n",
       "          -0.0000, -0.0000,  0.0000,  0.0000, -0.1916, -0.0000, -0.0000,\n",
       "          -0.0000,  0.0000,  0.0000,  0.0000, -0.0820, -0.0668,  0.0000,\n",
       "          -0.0000,  0.0000, -0.0722, -0.0000,  0.0000, -0.0000,  0.0000,\n",
       "           0.0000, -0.0907, -0.0000,  0.0000,  0.0000, -0.1824,  0.0000,\n",
       "           0.0000,  0.0000, -0.0687,  0.0000, -0.0744, -0.0000, -0.0000,\n",
       "          -0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0078, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000,  0.0000,  0.0380, -0.0000, -0.0000,  0.0000,\n",
       "          -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
       "           0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000, -0.0352,  0.0185,  0.0000,  0.0000,  0.0000,\n",
       "          -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0273, -0.0869, -0.0000,\n",
       "          -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0251,\n",
       "          -0.0577, -0.0042,  0.0006, -0.0000,  0.0283, -0.0057,  0.0000,\n",
       "           0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0170,\n",
       "          -0.0000,  0.0000,  0.0380, -0.0000,  0.0000, -0.0000,  0.0149,\n",
       "           0.0085, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0363,\n",
       "          -0.0000,  0.0000, -0.0000,  0.0026, -0.0000,  0.0000, -0.0000,\n",
       "           0.0000,  0.0000, -0.0000,  0.0003,  0.0000,  0.0211, -0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0264, -0.0000,  0.0000,\n",
       "           0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
       "          -0.0000,  0.0548, -0.0000,  0.0155,  0.0000,  0.0000, -0.0143,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0400, -0.0112,\n",
       "           0.0026,  0.0000,  0.0000,  0.0000,  0.0173,  0.0000,  0.0000,\n",
       "          -0.0484,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0220,\n",
       "          -0.0000, -0.0000,  0.0000,  0.0000,  0.0935, -0.0715, -0.0356,\n",
       "          -0.0000,  0.0000, -0.0000, -0.0086,  0.0000,  0.0000, -0.0000,\n",
       "           0.0336, -0.0000,  0.0000,  0.0000,  0.0193, -0.0000, -0.0000,\n",
       "           0.0379,  0.0237,  0.0000, -0.0072,  0.0000,  0.0000,  0.0928,\n",
       "          -0.0462,  0.0000, -0.0078,  0.0000,  0.0000,  0.0000, -0.0000,\n",
       "          -0.0000,  0.0000,  0.0000, -0.0209, -0.0092, -0.0000, -0.0304,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0173,  0.0000,  0.0267,\n",
       "          -0.0566,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0818,\n",
       "          -0.0462,  0.0000,  0.0386,  0.0226],\n",
       "         [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0057,  0.0000, -0.0000,\n",
       "           0.0000,  0.0000,  0.0567, -0.0000,  0.0000, -0.0000, -0.0083,\n",
       "          -0.0199, -0.0000,  0.0000, -0.0000, -0.0530,  0.0000,  0.0000,\n",
       "           0.0083,  0.0000, -0.0000, -0.0067,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
       "           0.0214, -0.0000, -0.0000, -0.0351,  0.0000,  0.0000, -0.0000,\n",
       "          -0.0351,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
       "          -0.0000,  0.0310,  0.0221,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "           0.0000,  0.0000, -0.0000, -0.0271, -0.0000, -0.0000,  0.0000,\n",
       "          -0.0000,  0.0000,  0.0000,  0.0506,  0.0000, -0.0063, -0.0000,\n",
       "           0.0000,  0.0252, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          -0.0000,  0.0000, -0.0547, -0.0112, -0.0000,  0.0000, -0.0000,\n",
       "           0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0450,  0.0103,\n",
       "          -0.0000, -0.0682,  0.0000, -0.0000, -0.0172, -0.0000,  0.0000,\n",
       "          -0.0841,  0.0000, -0.0000,  0.0000, -0.0000,  0.0399,  0.0000,\n",
       "          -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "           0.0475, -0.0000,  0.0194, -0.0000, -0.0000,  0.0318, -0.0000,\n",
       "           0.0000,  0.0193,  0.0000, -0.0000,  0.0049, -0.0000, -0.0000,\n",
       "           0.0000,  0.0000, -0.1172,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "           0.0000, -0.0135, -0.0000,  0.0373, -0.0168,  0.0604, -0.0000,\n",
       "           0.0000,  0.0000, -0.0000, -0.0138, -0.0000,  0.0000,  0.0000,\n",
       "          -0.0061, -0.0788,  0.0000,  0.0000, -0.0131, -0.0000,  0.0000,\n",
       "          -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0066, -0.0000,\n",
       "           0.0000,  0.0048,  0.0000, -0.0387, -0.0000, -0.0000, -0.0000,\n",
       "           0.0342, -0.0000, -0.0000, -0.0000, -0.0000, -0.0124,  0.0000,\n",
       "           0.0016, -0.0000, -0.0682,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "           0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0031, -0.0000,\n",
       "           0.0000, -0.0154, -0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0698,  0.0000, -0.0749,  0.0000, -0.1378,  0.0000,\n",
       "           0.0000, -0.0977, -0.0000, -0.0000,  0.0000, -0.0118, -0.0000,\n",
       "          -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0095, -0.1241,\n",
       "          -0.0000,  0.0000, -0.1064,  0.0000, -0.0393, -0.2126,  0.0000,\n",
       "           0.0000,  0.0000, -0.0000, -0.0000, -0.1017, -0.0728,  0.0000,\n",
       "           0.0000, -0.0922,  0.0000,  0.0000, -0.0440, -0.1449,  0.0000,\n",
       "           0.0000, -0.0440, -0.1449,  0.0000, -0.0000, -0.0000,  0.0000,\n",
       "          -0.0359, -0.0000,  0.0000, -0.1606,  0.0000,  0.0000, -0.0000,\n",
       "          -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.1016,\n",
       "           0.0000, -0.0035, -0.1023, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          -0.1468,  0.0000, -0.0000,  0.0000, -0.0836, -0.0073,  0.0000,\n",
       "          -0.0000, -0.0569,  0.0000,  0.0000, -0.0296, -0.0794,  0.0000,\n",
       "          -0.0138, -0.0189, -0.0460,  0.0000, -0.0559,  0.0000, -0.0000,\n",
       "          -0.0000,  0.0000, -0.0329, -0.0000,  0.0000,  0.0000, -0.1130,\n",
       "           0.0000, -0.0377, -0.1354,  0.0000, -0.0000,  0.0000, -0.1403,\n",
       "           0.0000,  0.0000, -0.0980, -0.1640,  0.0000,  0.0000, -0.1186,\n",
       "          -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
       "          -0.1043, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "           0.0000, -0.0918,  0.0000, -0.0909, -0.0000,  0.0000, -0.0694,\n",
       "           0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "           0.0000, -0.0106, -0.1218,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "           0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0679,\n",
       "           0.0000, -0.0809, -0.0985,  0.0000, -0.0000, -0.0112, -0.0662,\n",
       "          -0.0000, -0.0000, -0.1073, -0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.1077,\n",
       "           0.0000, -0.0710,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          -0.0000,  0.0000,  0.0000, -0.0972, -0.0000,  0.0000, -0.0000,\n",
       "           0.0000, -0.1008,  0.0000, -0.0662,  0.0000,  0.0000, -0.0000,\n",
       "          -0.0000,  0.0000, -0.0000, -0.0000]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer = Explainer(model)\n",
    "\n",
    "# Select a random sample from the test dataset\n",
    "sample_index = np.random.randint(len(datasets['test']))\n",
    "input_sequence, target_label = datasets['test'][sample_index]\n",
    "saliency_scores = explainer.saliency_map(input_sequence.unsqueeze(0))\n",
    "saliency_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5428d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = Explainer(model)\n",
    "saliency_scores = []\n",
    "for inputs, labels in loaders['test']:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    saliency_scores.append(explainer.saliency_map(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80fb9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0359, -0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0137, -0.0000,  ..., -0.0000,  0.0317, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0284,  ..., -0.0861,  0.0000, -0.0388]],\n",
       "\n",
       "        [[ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0449, -0.0907],\n",
       "         [ 0.0000, -0.0083, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0277, -0.0000, -0.0022,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ..., -0.1491,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0219, -0.0000],\n",
       "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0882,  0.0235,  0.0000,  ...,  0.0000, -0.0000,  0.0008],\n",
       "         [-0.0000,  0.0000,  0.0008,  ..., -0.0830,  0.0000, -0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0086,  0.0000, -0.0000,  ..., -0.0838, -0.0098,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0290,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0473,  0.0000,  ...,  0.0000,  0.0000, -0.0304]],\n",
       "\n",
       "        [[-0.0229, -0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0286, -0.0000,  ...,  0.0118, -0.0753,  0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0303,  ..., -0.0000,  0.0000, -0.0296]],\n",
       "\n",
       "        [[-0.0063, -0.0911,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0270,  0.0316,  0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.1206,  ..., -0.0000, -0.0000,  0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saliency_scores)\n",
    "saliency_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74f4133",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [50, 32, 4, 200]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m loaders[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m     saliency_scores\u001b[39m.\u001b[39mappend(explainer\u001b[39m.\u001b[39msmoothgrad(inputs))\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(saliency_scores[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/fall_co-op_2023/experiments/../src/explain/explainer.py:59\u001b[0m, in \u001b[0;36mExplainer.smoothgrad\u001b[0;34m(self, x, num_samples)\u001b[0m\n\u001b[1;32m     56\u001b[0m     perturbed_samples[i] \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m noise\n\u001b[1;32m     58\u001b[0m \u001b[39m# Calculate the saliency maps for the perturbed samples\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m perturbed_saliency_maps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaliency_map(perturbed_samples)\n\u001b[1;32m     61\u001b[0m \u001b[39m# Average the saliency maps over the num_samples\u001b[39;00m\n\u001b[1;32m     62\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(perturbed_saliency_maps, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/fall_co-op_2023/experiments/../src/explain/explainer.py:28\u001b[0m, in \u001b[0;36mExplainer.saliency_map\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mArguments\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m---------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mscores <torch.tensor> - The saliency scores for each feature in each sample; same shape as input. \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m x\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x)  \u001b[39m# shape (batch_size, num_classes)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m y \u001b[39m=\u001b[39m y[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_idx]  \u001b[39m# shape (batch_size,)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# Create a tensor with ones to match the shape of y for grad_outputs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/fall_co-op_2023/experiments/../src/models/deepbind_cnn.py:15\u001b[0m, in \u001b[0;36mDeepBindCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n\u001b[1;32m     16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(x, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# Global Average Pooling\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    310\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [50, 32, 4, 200]"
     ]
    }
   ],
   "source": [
    "explainer = Explainer(model)\n",
    "saliency_scores = []\n",
    "for inputs, labels in loaders['test']:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    saliency_scores.append(explainer.smoothgrad(inputs))\n",
    "print(saliency_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "babe5e88",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [50, 32, 4, 200]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m loaders[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m     saliency_scores\u001b[39m.\u001b[39mappend(explainer\u001b[39m.\u001b[39mintegrated_gradients(inputs))\n",
      "File \u001b[0;32m~/Documents/fall_co-op_2023/experiments/../src/explain/explainer.py:108\u001b[0m, in \u001b[0;36mExplainer.integrated_gradients\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m interpolated_inputs \u001b[39m=\u001b[39m baseline \u001b[39m+\u001b[39m alpha \u001b[39m*\u001b[39m (x \u001b[39m-\u001b[39m baseline)\n\u001b[1;32m    107\u001b[0m \u001b[39m# Calculate the saliency maps for each interpolated input\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m saliency_maps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaliency_map(interpolated_inputs)\n\u001b[1;32m    110\u001b[0m \u001b[39m# Integrate the saliency maps along the path (using the trapezoidal rule)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m integrated_gradients \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(saliency_maps, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m (x \u001b[39m-\u001b[39m baseline)\n",
      "File \u001b[0;32m~/Documents/fall_co-op_2023/experiments/../src/explain/explainer.py:28\u001b[0m, in \u001b[0;36mExplainer.saliency_map\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mArguments\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m---------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mscores <torch.tensor> - The saliency scores for each feature in each sample; same shape as input. \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m x\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x)  \u001b[39m# shape (batch_size, num_classes)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m y \u001b[39m=\u001b[39m y[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_idx]  \u001b[39m# shape (batch_size,)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# Create a tensor with ones to match the shape of y for grad_outputs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/fall_co-op_2023/experiments/../src/models/deepbind_cnn.py:15\u001b[0m, in \u001b[0;36mDeepBindCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n\u001b[1;32m     16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(x, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# Global Average Pooling\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    310\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [50, 32, 4, 200]"
     ]
    }
   ],
   "source": [
    "explainer = Explainer(model)\n",
    "saliency_scores = []\n",
    "for inputs, labels in loaders['test']:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    saliency_scores.append(explainer.integrated_gradients(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817976f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
